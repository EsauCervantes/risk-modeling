{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc44105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm   # gives you Φ (cdf) and Φ^{-1} (ppf)\n",
    "import matplotlib.pyplot as plt # optional, only if you want a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aea94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training CSV\n",
    "df = pd.read_csv(\"data/GiveMeSomeCredit/cs-training.csv\")\n",
    "\n",
    "# Quick look at the first rows\n",
    "#print(df.head())\n",
    "\n",
    "# Check the shape (#rows, #columns)\n",
    "#print(df.shape)\n",
    "\n",
    "# Inspect column names\n",
    "#print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fe10623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                  0\n",
      "SeriousDlqin2yrs                            0\n",
      "RevolvingUtilizationOfUnsecuredLines        0\n",
      "age                                         0\n",
      "NumberOfTime30-59DaysPastDueNotWorse        0\n",
      "DebtRatio                                   0\n",
      "MonthlyIncome                           29731\n",
      "NumberOfOpenCreditLinesAndLoans             0\n",
      "NumberOfTimes90DaysLate                     0\n",
      "NumberRealEstateLoansOrLines                0\n",
      "NumberOfTime60-89DaysPastDueNotWorse        0\n",
      "NumberOfDependents                       3924\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How many defaults vs non-defaults?\n",
    "#df[\"SeriousDlqin2yrs\"].value_counts()\n",
    "\n",
    "# Basic stats for income (often has missing values)\n",
    "#print(df[\"MonthlyIncome\"].describe())\n",
    "\n",
    "# Check for missing data\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8057de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b756cb97-0b59-4ba0-a75e-ad4f172b831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0.0,Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1.0/(1.0+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ec741e4-b019-4a44-a4ef-2ffddc6302ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(d,H,rng):\n",
    "    W1 = rng.normal(loc=0.0, scale = np.sqrt(2.0/d),size = (d,H) )\n",
    "    b1 = np.zeros((1,H))\n",
    "    W2 = rng.normal(loc = 0.0, scale = np.sqrt(2.0/H), size = (H,1) )\n",
    "    b2 = np.zeros((1,1))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21ba38bc-a969-4e29-bfdc-65702efaec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = X@W1 + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1@W2 + b2\n",
    "    Yhat = sigmoid(Z2)\n",
    "    cache = (X, Z1, A1, Z2, Yhat)\n",
    "    return Yhat, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecbec21c-ae8d-4812-8683-82d770502ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(Yhat, y, eps=1e-12):\n",
    "    Yhat = np.clip(Yhat, eps, 1.0 - eps)\n",
    "    N = y.shape[0]\n",
    "    loss = -(y * np.log(Yhat) + (1-y) * np.log(1 - Yhat) ).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fdd0924-247f-43fb-ac1b-ca7943e831c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d, H = 10, 64\n",
    "W1, b1, W2, b2 = he_init(d, H, rng)\n",
    "\n",
    "X = rng.normal(size=(32, d))\n",
    "y = rng.integers(low=0, high=2, size=(32,1))\n",
    "\n",
    "Yhat, cache = forward(X, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "006faef1-2daf-46da-be0e-ff38241a0df3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# number of features\u001b[39;00m\n\u001b[1;32m      3\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m      \n\u001b[1;32m      4\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m he_init(d, H, rng)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "d = .shape[1]   # number of features\n",
    "H = 64      \n",
    "W1, b1, W2, b2 = he_init(d, H, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "271c6b16-66a1-4abf-ab1a-10332d15afd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions consistently here (no hard-coding)\n",
    "           # hidden units (you can try 32 or 128 later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232a7776-3474-44d3-a247-0bad08003983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat shape (32, 1)\n",
      "Loss: 1.2126950363957207\n"
     ]
    }
   ],
   "source": [
    "print(\"Yhat shape\", Yhat.shape)\n",
    "print(\"Loss:\", bce_loss(Yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107d4e94-26fc-4c65-a2bf-62ba2112a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X,y,cache,W1,b1,W2,b2):\n",
    "    Z1, A1, Z2, Yhat = cache\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    G2 = (Yhat-y)/N\n",
    "    \n",
    "    dW2 = A1.T@G2\n",
    "    db2 = np.sum(G2, axis = 0, keepdims = True)\n",
    "    \n",
    "    G1 = (G2@W2.T) * (Z1>0)\n",
    "    \n",
    "    dW1 = X.T@G1\n",
    "    db1 = np.sum(G1, axis=0, keepdims=True) # (1,H)\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f44a756-717e-4631-8e49-85c44b4910b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (120000, 11) val shape: (30000, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) load\n",
    "df = pd.read_csv(\"data/GiveMeSomeCredit/cs-training.csv\")\n",
    "\n",
    "# 2) target y (column is 0/1)\n",
    "y = df[\"SeriousDlqin2yrs\"].values.reshape(-1, 1)\n",
    "\n",
    "# 3) features X (drop target; keep only numeric columns)\n",
    "X = df.drop(columns=[\"SeriousDlqin2yrs\"])\n",
    "X = X.select_dtypes(include=[np.number]).values  # shape (N, d_raw)\n",
    "\n",
    "# 4) impute missing values with median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# 5) standardize features (mean 0, std 1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 6) train/val split (stratify preserves default rate)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "N_tr, d = X_tr.shape\n",
    "N_va = X_va.shape[0]\n",
    "print(\"train shape:\", X_tr.shape, \"val shape:\", X_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb23890-4cfd-4b2a-a3ec-05e8df606aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "H = 64  # you can try 32, 128 later\n",
    "\n",
    "W1, b1, W2, b2 = he_init(d, H, rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eac2711-7f35-416d-ba84-93c773e5614a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch  = 256\n",
    "lr     = 1e-3       # start modest; increase to 3e-3 if loss plateaus\n",
    "weight_decay = 1e-5 # L2 regularization on weights\n",
    "\n",
    "# (optional) class imbalance weight on positives:\n",
    "pos = y_tr.sum()\n",
    "neg = len(y_tr) - pos\n",
    "pos_weight = float(neg / max(pos, 1))   # e.g., ~10-20 typically\n",
    "use_pos_weight = True  # set False if you want plain BCE first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e46a3fd2-0b87-49f5-829f-1fb3742720af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bce_loss_weighted(Yhat, y, pos_weight=1.0, eps=1e-12):\n",
    "    Yhat = np.clip(Yhat, eps, 1.0 - eps)\n",
    "    # weight the positive term by pos_weight\n",
    "    loss = -(pos_weight * y * np.log(Yhat) + (1 - y) * np.log(1 - Yhat)).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6a30e31-40a5-452c-9625-c6bb9db23b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward_weighted(X, y, cache, W2, pos_weight=1.0):\n",
    "    Z1, A1, Z2, Yhat = cache\n",
    "    N = X.shape[0]\n",
    "    # start with standard error\n",
    "    G2 = (Yhat - y)\n",
    "    # up-weight positive-class errors approximately\n",
    "    G2[y.astype(bool)] *= pos_weight\n",
    "    G2 /= N\n",
    "    dW2 = A1.T @ G2\n",
    "    db2 = np.sum(G2, axis=0, keepdims=True)\n",
    "    G1 = (G2 @ W2.T) * (Z1 > 0)\n",
    "    dW1 = X.T @ G1\n",
    "    db1 = np.sum(G1, axis=0, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8969ac6d-7fff-4d7c-9abb-8cf26495a8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# backward (weighted or not)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pos_weight:\n\u001b[0;32m---> 26\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_weighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     loss_b \u001b[38;5;241m=\u001b[39m bce_loss_weighted(Yhat, yb, pos_weight)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36mbackward_weighted\u001b[0;34m(X, y, cache, W2, pos_weight)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_weighted\u001b[39m(X, y, cache, W2, pos_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     Z1, A1, Z2, Yhat \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m      3\u001b[0m     N \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# start with standard error\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "def eval_loss(Xe, ye, W1, b1, W2, b2, weighted=False, pos_w=1.0):\n",
    "    Yhat, _ = forward(Xe, W1, b1, W2, b2)\n",
    "    if weighted:\n",
    "        return bce_loss_weighted(Yhat, ye, pos_w)\n",
    "    else:\n",
    "        return bce_loss(Yhat, ye)\n",
    "\n",
    "hist = {\"train\": [], \"val\": []}\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    # shuffle training data each epoch\n",
    "    idx = rng.permutation(N_tr)\n",
    "    X_tr = X_tr[idx]; y_tr = y_tr[idx]\n",
    "\n",
    "    # iterate over mini-batches\n",
    "    for start in range(0, N_tr, batch):\n",
    "        stop = min(start + batch, N_tr)\n",
    "        Xb = X_tr[start:stop]\n",
    "        yb = y_tr[start:stop]\n",
    "\n",
    "        # forward\n",
    "        Yhat, cache = forward(Xb, W1, b1, W2, b2)\n",
    "\n",
    "        # backward (weighted or not)\n",
    "        if use_pos_weight:\n",
    "            dW1, db1, dW2, db2 = backward_weighted(Xb, yb, cache, W2, pos_weight)\n",
    "            loss_b = bce_loss_weighted(Yhat, yb, pos_weight)\n",
    "        else:\n",
    "            dW1, db1, dW2, db2 = backward(Xb, yb, cache, W2)\n",
    "            loss_b = bce_loss(Yhat, yb)\n",
    "\n",
    "        # L2 regularization on weights\n",
    "        dW1 += weight_decay * W1\n",
    "        dW2 += weight_decay * W2\n",
    "\n",
    "        # SGD update\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "\n",
    "    # end epoch: evaluate full train/val loss\n",
    "    tr_loss = eval_loss(X_tr, y_tr, W1, b1, W2, b2, weighted=use_pos_weight, pos_w=pos_weight)\n",
    "    va_loss = eval_loss(X_va, y_va, W1, b1, W2, b2, weighted=use_pos_weight, pos_w=pos_weight)\n",
    "    hist[\"train\"].append(tr_loss); hist[\"val\"].append(va_loss)\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"epoch {ep:02d} | train={tr_loss:.4f} | val={va_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44c8fa67-da4d-4517-af2d-c2a5de2cad4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train=0.3600 | val=0.3588\n",
      "epoch 05 | train=0.2579 | val=0.2582\n",
      "epoch 10 | train=0.2464 | val=0.2469\n",
      "epoch 15 | train=0.2438 | val=0.2442\n",
      "epoch 20 | train=0.2419 | val=0.2423\n",
      "epoch 25 | train=0.2404 | val=0.2406\n",
      "epoch 30 | train=0.2390 | val=0.2392\n",
      "First 10 validation PDs: [0.03671458 0.02298935 0.04932452 0.08167806 0.03269545 0.01250404\n",
      " 0.08270306 0.64588307 0.07126863 0.14015793]\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Imports & seed =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# ===== 1) Activation functions =====\n",
    "def relu(Z):\n",
    "    return np.maximum(0.0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    # numerically stable enough for this use\n",
    "    return 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "# ===== 2) He initialization (for ReLU) =====\n",
    "def he_init(d, H, rng):\n",
    "    W1 = rng.normal(0.0, np.sqrt(2.0/d), size=(d, H))\n",
    "    b1 = np.zeros((1, H))\n",
    "    W2 = rng.normal(0.0, np.sqrt(2.0/H), size=(H, 1))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# ===== 3) Forward pass =====\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    # X: (N, d)\n",
    "    Z1 = X @ W1 + b1            # (N, H) affine\n",
    "    A1 = relu(Z1)               # (N, H) nonlinearity\n",
    "    Z2 = A1 @ W2 + b2           # (N, 1) logit\n",
    "    Yhat = sigmoid(Z2)          # (N, 1) probability\n",
    "    cache = (Z1, A1, Z2, Yhat)  # we need these for backprop\n",
    "    return Yhat, cache\n",
    "\n",
    "# ===== 4) Binary cross-entropy loss (mean over batch) =====\n",
    "def bce_loss(Yhat, y, eps=1e-12):\n",
    "    Yhat = np.clip(Yhat, eps, 1.0 - eps)\n",
    "    return -(y * np.log(Yhat) + (1 - y) * np.log(1 - Yhat)).mean()\n",
    "\n",
    "# ===== 5) Backward pass (gradients via chain rule) =====\n",
    "def backward(X, y, cache, W2):\n",
    "    Z1, A1, Z2, Yhat = cache\n",
    "    N = X.shape[0]\n",
    "    G2 = (Yhat - y) / N               # (N, 1) output error\n",
    "    dW2 = A1.T @ G2                   # (H, 1)\n",
    "    db2 = np.sum(G2, axis=0, keepdims=True)   # (1, 1)\n",
    "    G1 = (G2 @ W2.T) * (Z1 > 0)       # (N, H) elementwise mask for ReLU\n",
    "    dW1 = X.T @ G1                    # (d, H)\n",
    "    db1 = np.sum(G1, axis=0, keepdims=True)   # (1, H)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# ===== 6) Load & prepare data (Give Me Some Credit) =====\n",
    "# Put the CSV at: data/cs-training.csv\n",
    "df = pd.read_csv(\"data/GiveMeSomeCredit/cs-training.csv\")\n",
    "\n",
    "# Target and features\n",
    "y_all = df[\"SeriousDlqin2yrs\"].values.reshape(-1, 1)  # (N,1)\n",
    "X_all = df.drop(columns=[\"SeriousDlqin2yrs\"])\n",
    "X_all = X_all.select_dtypes(include=[np.number]).values  # keep numeric only\n",
    "\n",
    "# Impute missing with median, then standardize\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "X_all = imputer.fit_transform(X_all)\n",
    "X_all = scaler.fit_transform(X_all)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "# Define dimensions consistently here (no hard-coding)\n",
    "d = X_train.shape[1]   # number of features\n",
    "H = 64                 # hidden units (you can try 32 or 128 later)\n",
    "\n",
    "# ===== 7) Initialize parameters =====\n",
    "W1, b1, W2, b2 = he_init(d, H, rng)\n",
    "\n",
    "# ===== 8) Training loop (mini-batch SGD) =====\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "lr = 1e-3   # try 3e-3 if loss plateaus; 3e-4 if it oscillates\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "for ep in range(1, epochs + 1):\n",
    "    # shuffle indices for this epoch\n",
    "    idx = rng.permutation(n_train)\n",
    "\n",
    "    # iterate over mini-batches by index (no in-place array shuffling)\n",
    "    for start in range(0, n_train, batch_size):\n",
    "        stop = min(start + batch_size, n_train)\n",
    "        bidx = idx[start:stop]\n",
    "        Xb = X_train[bidx]\n",
    "        yb = y_train[bidx]\n",
    "\n",
    "        # forward -> loss\n",
    "        Yhat_b, cache_b = forward(Xb, W1, b1, W2, b2)\n",
    "        loss_b = bce_loss(Yhat_b, yb)\n",
    "\n",
    "        # backward -> grads\n",
    "        dW1, db1, dW2, db2 = backward(Xb, yb, cache_b, W2)\n",
    "\n",
    "        # SGD update\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "\n",
    "    # end-epoch: evaluate full train/val loss\n",
    "    train_pred, _ = forward(X_train, W1, b1, W2, b2)\n",
    "    val_pred, _   = forward(X_val,   W1, b1, W2, b2)\n",
    "    train_loss = bce_loss(train_pred, y_train)\n",
    "    val_loss   = bce_loss(val_pred,   y_val)\n",
    "\n",
    "    if ep % 5 == 0 or ep == 1:\n",
    "        print(f\"epoch {ep:02d} | train={train_loss:.4f} | val={val_loss:.4f}\")\n",
    "\n",
    "# ===== 9) Example predictions (probabilities) =====\n",
    "probs_val = val_pred.ravel()\n",
    "print(\"First 10 validation PDs:\", probs_val[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744898a-0493-47d7-89a6-1cfedd35cd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
